import { Audio, computeFFT, computeFFTFromIR, db, FFTResult, groupDelays, ImpulseResponseResult, rms, smoothFFT, twoChannelImpulseResponse } from "./audio";
import { Farina, plotDistortion, plotTHD } from "./farina";
import { storage } from "./storage";
import { audio } from "./audio";
import "./device-settings";ith getUserMedia</title>
import { linspace, max } from "./math";h=device-width, initial-scale=1">
import { COLORS, plot } from "./plotting";
        body {
console.debug("App module loaded");, Arial, sans-serif;
            background: #f6f8fa;
const root = document.documentElement;
const uiColor = "#0366d6";
root.style.setProperty('--color', uiColor);
        }
let tabCounter = 0;
const tabsContainer = document.getElementById('tabs-outer') as HTMLElement;
const tabsInnerContainer = document.getElementById('tabs') as HTMLElement;
const tabContents = document.getElementById('tab-contents') as HTMLElement;
const responseFileUploadInput = document.getElementById('responseFileUpload') as HTMLInputElement;
const referenceFileUploadInput = document.getElementById('referenceFileUpload') as HTMLInputElement;
const analyzeUploadBtn = document.getElementById('analyzeUploadBtn') as HTMLButtonElement;
            display: block;
// Polar upload controls
const polarReferenceFileInput = document.getElementById('polarReferenceFile') as HTMLInputElement;
const polarMeasurementsEl = document.getElementById('polarMeasurements') as HTMLElement;
const addPolarMeasurementBtn = document.getElementById('addPolarMeasurementBtn') as HTMLButtonElement;         border-radius: 1rem;
const analyzePolarBtn = document.getElementById('analyzePolarBtn') as HTMLButtonElement;            background: #e0e0e0;
const polarStatusEl = document.getElementById('polarStatus') as HTMLElement;

// Enable analyze button when response file is selected
responseFileUploadInput.addEventListener('change', () => {            transition: background 0.2s, color 0.2s;
    analyzeUploadBtn.disabled = !responseFileUploadInput.files?.length;
});

type PolarMeasurement = { angleDeg: number; file: File };

function normalizeAngleDeg(angleDeg: number): number {
    let a = angleDeg % 360;;
    if (a < 0) a += 360;           justify-content: center;
    return a;            gap: 1rem;
}

function getPolarMeasurements(): PolarMeasurement[] {
    const rows = Array.from(polarMeasurementsEl.querySelectorAll<HTMLElement>('.polar-measurement-row'));7rem 2rem;
    const out: PolarMeasurement[] = [];em;
    for (const row of rows) {
        const angleInput = row.querySelector<HTMLInputElement>('.polar-angle');ius: 0.5rem;
        const fileInput = row.querySelector<HTMLInputElement>('.polar-response-file');          background: #1976d2;
        if (!angleInput || !fileInput) continue;            color: #fff;

        const file = fileInput.files?.[0];
        if (!file) continue;

        const parsed = parseFloat(angleInput.value);
        const angleDeg = Number.isFinite(parsed) ? normalizeAngleDeg(parsed) : 0;
        out.push({ angleDeg, file });
    }
    return out;
}

function updatePolarAnalyzeEnabled(): void {
    const hasReference = !!polarReferenceFileInput.files?.length;
    const hasAnyMeasurement = getPolarMeasurements().length > 0;
    analyzePolarBtn.disabled = !(hasReference && hasAnyMeasurement);

    if (!hasReference) {        .controls h2 {
        polarStatusEl.textContent = 'Select a reference/stimulus file.';
    } else if (!hasAnyMeasurement) {
        polarStatusEl.textContent = 'Add at least one measurement (angle + file).';
    } else {       color: #1976d2;
        polarStatusEl.textContent = '';
    }
}   .controls label {

function addPolarMeasurementRow(initialAngleDeg: number = 0): void {           align-items: center;
    const row = document.createElement('div');            gap: 0.5rem;
    row.className = 'param-row polar-measurement-row';
    row.innerHTML = `   font-size: 1rem;
        <label>Angle (deg):</label>
        <input type="number" class="param-input polar-angle" value="${initialAngleDeg}" step="1" min="0" max="360">
        <input type="file" class="polar-response-file" accept="audio/*,.wav,.mp3,.flac,.ogg">
        <button class="button-custom button-custom-secondary polar-remove" type="button">Remove</button> accent-color: #1976d2;
    `;    width: 1.2em;
    polarMeasurementsEl.appendChild(row);
}

// Polar UI wiring
polarReferenceFileInput.addEventListener('change', updatePolarAnalyzeEnabled);<body>
addPolarMeasurementBtn.addEventListener('click', () => {
    addPolarMeasurementRow(0);
    updatePolarAnalyzeEnabled(); class="button-group">
});
polarMeasurementsEl.addEventListener('input', (e) => {
    const t = e.target as HTMLElement;    </div>
    if (t.classList.contains('polar-angle')) updatePolarAnalyzeEnabled();
});
polarMeasurementsEl.addEventListener('change', (e) => {
    const t = e.target as HTMLElement;
    if (t.classList.contains('polar-response-file')) updatePolarAnalyzeEnabled();n>
});
polarMeasurementsEl.addEventListener('click', (e) => {
    const t = e.target as HTMLElement;ppression">
    if (!t.classList.contains('polar-remove')) return;   <span>Noise Suppression</span>
    const row = t.closest('.polar-measurement-row');        </label>
    row?.remove();
    if (polarMeasurementsEl.querySelectorAll('.polar-measurement-row').length === 0) {ox" id="echoCancellation">
        addPolarMeasurementRow(0);
    }
    updatePolarAnalyzeEnabled();
});pt>
= null;
updatePolarAnalyzeEnabled();

// ============================================================================
// Acquisition Tab Functionality       function setStatus(running) {
// ============================================================================            const statusEl = document.getElementById('status');

interface AcquisitionState {
    audioContext: AudioContext | null;ning');
    mediaRecorder: MediaRecorder | null;
    recordedChunks: Blob[];ent = 'Stopped';
    oscillatorNode: OscillatorNode | null;           statusEl.classList.remove('running');
    playbackSource: AudioBufferSourceNode | null;         }
    isRecording: boolean;        }
}
sync function startLoopback() {
const acquisitionState: AcquisitionState = {gainControl').checked;
    audioContext: null,('noiseSuppression').checked;
    mediaRecorder: null,ancellation').checked;
    recordedChunks: [],
    oscillatorNode: null,
    playbackSource: null,
    isRecording: false
};  stream = await navigator.mediaDevices.getUserMedia({
         audio: {
const startBtn = document.getElementById('startBtn') as HTMLButtonElement;                autoGainControl,
const stopBtn = document.getElementById('stopBtn') as HTMLButtonElement;pression,
const playBtn = document.getElementById('playBtn') as HTMLButtonElement;
const stopPlayBtn = document.getElementById('stopPlayBtn') as HTMLButtonElement;
const sweepStartFreqInput = document.getElementById('sweepStartFreq') as HTMLInputElement;
const sweepEndFreqInput = document.getElementById('sweepEndFreq') as HTMLInputElement;                audioCtx = new (window.AudioContext || window.webkitAudioContext)();
const sweepDurationInput = document.getElementById('sweepDuration') as HTMLInputElement;
const recordingStatusEl = document.getElementById('recordingStatus') as HTMLElement;
const recordingMeterEl = document.getElementById('recordingMeter') as HTMLElement;      setStatus(true);
const recordingVisualizationEl = document.getElementById('recordingVisualization') as HTMLElement;            } catch (err) {
const recordedAudioContainer = document.getElementById('recordedAudioContainer') as HTMLElement;ssage);
const recordedAudioEl = document.getElementById('recordedAudio') as HTMLAudioElement;
const analyzeRecordingBtn = document.getElementById('analyzeRecordingBtn') as HTMLButtonElement;
const viewWaveformBtn = document.getElementById('viewWaveformBtn') as HTMLButtonElement;
const channelSelectionContainer = document.getElementById('channelSelectionContainer') as HTMLElement;
const channelSelect = document.getElementById('channelSelect') as HTMLSelectElement;
  if (source) {
async function initializeAudioContext(): Promise<AudioContext> {                source.disconnect();
    if (!acquisitionState.audioContext) {
        acquisitionState.audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
    }
    if (acquisitionState.audioContext.state === 'suspended') {
        await acquisitionState.audioContext.resume();        audioCtx = null;
    }
    return acquisitionState.audioContext;
}

async function detectAndSetupChannels(): Promise<void> {            }
    try {
        // Request audio input to check channel count        }
        const stream = await navigator.mediaDevices.getUserMedia({ 
            audio: { echoCancellation: false, noiseSuppression: false, autoGainControl: false } 
        });oopback;
        
        const audioContext = await initializeAudioContext();        ['gainControl', 'noiseSuppression', 'echoCancellation'].forEach(id => {
        const analyser = audioContext.createAnalyser();EventListener('change', () => {
        const source = audioContext.createMediaStreamSource(stream);
        source.connect(analyser);
                }
        // Get channel count from the stream
        const channelCount = source.mediaStream.getAudioTracks()[0].getSettings()?.channelCount || 1;
        
        // Stop the stream as we only need it for channel detection
        stream.getTracks().forEach(track => track.stop());    </script>

        // Populate channel selection
        channelSelect.innerHTML = '';        for (let i = 0; i < channelCount; i++) {            const option = document.createElement('option');            option.value = i.toString();            const channelNames = ['Left', 'Right', 'Center', 'LFE', 'Back Left', 'Back Right'];            option.textContent = `Channel ${i + 1}${channelNames[i] ? ` (${channelNames[i]})` : ''}`;            channelSelect.appendChild(option);        }        // Show channel selection only if more than 1 channel        if (channelCount > 1) {            channelSelectionContainer.style.display = 'flex';        } else {            channelSelectionContainer.style.display = 'none';        }    } catch (error) {        console.error('Error detecting channels:', error);        channelSelectionContainer.style.display = 'none';    }}// Detect channels when the acquisition tab is openedtabsContainer.addEventListener('click', (e: MouseEvent) => {    const target = e.target as HTMLElement;    if (target.classList.contains('tab') && target.dataset.tab === 'acquisition') {        detectAndSetupChannels();    }});async function startRecordingAndPlayback(): Promise<void> {    try {        // Initialize audio context and get microphone stream        const audioContext = await initializeAudioContext();        const stream = await navigator.mediaDevices.getUserMedia({             audio: {                 echoCancellation: false,                 noiseSuppression: false,                 autoGainControl: false             }         });                // Create media recorder        acquisitionState.recordedChunks = [];        acquisitionState.mediaRecorder = new MediaRecorder(stream);        acquisitionState.isRecording = true;        acquisitionState.mediaRecorder.ondataavailable = (e: BlobEvent) => {            acquisitionState.recordedChunks.push(e.data);        };        acquisitionState.mediaRecorder.onstop = async () => {            const recordedBlob = new Blob(acquisitionState.recordedChunks, { type: 'audio/wav' });            const url = URL.createObjectURL(recordedBlob);            recordedAudioEl.src = url;            recordedAudioContainer.style.display = 'block';            recordingVisualizationEl.style.display = 'none';        };        // Generate and play sweep        const startFreq = parseFloat(sweepStartFreqInput.value);        const endFreq = parseFloat(sweepEndFreqInput.value);        const duration = parseFloat(sweepDurationInput.value);                // Pre and post recording buffers (in seconds)        const preRecordTime = 0.5;  // Start recording 0.5s before playback        const postRecordTime = 1.0; // Continue recording 1s after playback ends        const totalRecordTime = preRecordTime + duration + postRecordTime;        const [sweepSignal, , ] = audio.chirp(startFreq, endFreq, duration);        // Create audio buffer from sweep signal        const audioBuffer = audioContext.createBuffer(1, sweepSignal.length, audioContext.sampleRate);        const channelData = audioBuffer.getChannelData(0);        channelData.set(sweepSignal);        // Create gain nodes for monitoring        const sourceGain = audioContext.createGain();        sourceGain.gain.value = 0.5;        // Start recording        recordingStatusEl.textContent = `Recording for ${totalRecordTime.toFixed(1)}s...`;        recordingVisualizationEl.style.display = 'block';        acquisitionState.mediaRecorder.start();        // Update UI        startBtn.disabled = true;        stopBtn.disabled = false;        playBtn.disabled = true;        sweepStartFreqInput.disabled = true;        sweepEndFreqInput.disabled = true;        sweepDurationInput.disabled = true;        // Start playback after pre-record time        setTimeout(() => {            acquisitionState.playbackSource = audioContext.createBufferSource();            acquisitionState.playbackSource.buffer = audioBuffer;            acquisitionState.playbackSource.connect(sourceGain);            sourceGain.connect(audioContext.destination);            acquisitionState.playbackSource.start();        }, preRecordTime * 1000);        // Stop recording after total time (pre + sweep + post)        setTimeout(() => {            stopRecording();        }, totalRecordTime * 1000);    } catch (error) {        console.error('Error starting recording:', error);        recordingStatusEl.textContent = `Error: ${(error as Error).message}`;        recordingStatusEl.style.color = '#d73a49';    }}async function playbackOnly(): Promise<void> {    try {        const audioContext = await initializeAudioContext();                const startFreq = parseFloat(sweepStartFreqInput.value);        const endFreq = parseFloat(sweepEndFreqInput.value);        const duration = parseFloat(sweepDurationInput.value);        const [sweepSignal] = audio.chirp(startFreq, endFreq, duration);        // Create audio buffer from sweep signal        const audioBuffer = audioContext.createBuffer(1, sweepSignal.length, audioContext.sampleRate);        const channelData = audioBuffer.getChannelData(0);        channelData.set(sweepSignal);        // Create gain node        const sourceGain = audioContext.createGain();        sourceGain.gain.value = 0.5;        // Connect and start playback        acquisitionState.playbackSource = audioContext.createBufferSource();        acquisitionState.playbackSource.buffer = audioBuffer;        acquisitionState.playbackSource.connect(sourceGain);        sourceGain.connect(audioContext.destination);        acquisitionState.playbackSource.start();        recordingStatusEl.textContent = `Playing sweep...`;        recordingStatusEl.style.color = '#0366d6';        playBtn.disabled = true;        stopPlayBtn.disabled = false;        setTimeout(() => {            stopPlayback();        }, (duration + 0.5) * 1000);    } catch (error) {        console.error('Error during playback:', error);        recordingStatusEl.textContent = `Error: ${(error as Error).message}`;        recordingStatusEl.style.color = '#d73a49';    }}function stopRecording(): void {    if (acquisitionState.mediaRecorder && acquisitionState.isRecording) {        acquisitionState.mediaRecorder.stop();        acquisitionState.isRecording = false;        // Stop microphone stream        acquisitionState.mediaRecorder.stream.getTracks().forEach(track => track.stop());        recordingStatusEl.textContent = 'Recording complete. Ready to analyze.';        recordingStatusEl.style.color = '#28a745';    }    if (acquisitionState.playbackSource) {        acquisitionState.playbackSource.stop();    }    // Update UI    startBtn.disabled = false;    stopBtn.disabled = true;    playBtn.disabled = false;    sweepStartFreqInput.disabled = false;    sweepEndFreqInput.disabled = false;    sweepDurationInput.disabled = false;}function stopPlayback(): void {    if (acquisitionState.playbackSource) {        try {            acquisitionState.playbackSource.stop();        } catch (e) {            // Already stopped        }    }    recordingStatusEl.textContent = 'Playback stopped.';    playBtn.disabled = false;    stopPlayBtn.disabled = true;}// Event listeners for acquisition controlsstartBtn.addEventListener('click', startRecordingAndPlayback);stopBtn.addEventListener('click', stopRecording);playBtn.addEventListener('click', playbackOnly);stopPlayBtn.addEventListener('click', stopPlayback);analyzeRecordingBtn.addEventListener('click', async () => {    if (!recordedAudioEl.src) return;    try {        const audioContext = await initializeAudioContext();        const response = await fetch(recordedAudioEl.src);        const arrayBuffer = await response.arrayBuffer();        const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);                // Extract the selected channel        const selectedChannel = parseInt(channelSelect.value, 10);        let recordedAudio: Audio;                if (audioBuffer.numberOfChannels > 1 && selectedChannel < audioBuffer.numberOfChannels) {            // Multi-channel recording: extract selected channel            const channelData = audioBuffer.getChannelData(selectedChannel);            recordedAudio = Audio.fromSamples(channelData, audioBuffer.sampleRate);        } else {            // Single channel or default to first channel            recordedAudio = Audio.fromAudioBuffer(audioBuffer);        }        // Generate the chirp sweep as reference data        const startFreq = parseFloat(sweepStartFreqInput.value);        const endFreq = parseFloat(sweepEndFreqInput.value);        const duration = parseFloat(sweepDurationInput.value);        const [sweepSignal] = audio.chirp(startFreq, endFreq, duration);        const referenceAudio = Audio.fromSamples(sweepSignal, audioContext.sampleRate);        // Add timestamp to recording name        const now = new Date();        const dateTime = now.toLocaleString('sv-SE', {             year: '2-digit',             month: '2-digit',             day: '2-digit',            hour: '2-digit',            minute: '2-digit',            second: '2-digit',            hour12: false        }).replace(',', '');        const recordingName = `${dateTime}`;        createAnalysisTab(            recordedAudio.applyGain(1 / 16384),            referenceAudio.applyGain(1 / 16384),            recordingName,            `${startFreq}-${endFreq}Hz`,        );        //             } catch (error) {        console.error('Error analyzing recording:', error);        alert('Error analyzing recording: ' + (error as Error).message);    }});viewWaveformBtn.addEventListener('click', async () => {    if (!recordedAudioEl.src) return;    try {        const audioContext = await initializeAudioContext();        const response = await fetch(recordedAudioEl.src);        const arrayBuffer = await response.arrayBuffer();        const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);                // Extract the selected channel        const selectedChannel = parseInt(channelSelect.value, 10);        let recordedAudio: Audio;                if (audioBuffer.numberOfChannels > 1 && selectedChannel < audioBuffer.numberOfChannels) {            // Multi-channel recording: extract selected channel            const channelData = audioBuffer.getChannelData(selectedChannel);            recordedAudio = Audio.fromSamples(channelData, audioBuffer.sampleRate);        } else {            // Single channel or default to first channel            recordedAudio = Audio.fromAudioBuffer(audioBuffer);        }        // Add timestamp to tab name        const now = new Date();        const dateTime = now.toLocaleString('sv-SE', {             year: '2-digit',             month: '2-digit',             day: '2-digit',            hour: '2-digit',            minute: '2-digit',            second: '2-digit',            hour12: false        }).replace(',', '');        const recordingName = `${dateTime}`;        // Create analysis tab with only the recorded audio (no reference)        createAnalysisTab(            recordedAudio.applyGain(1 / 16384),            null,            recordingName,            'Waveform View'        );    } catch (error) {        console.error('Error viewing waveform:', error);        alert('Error viewing waveform: ' + (error as Error).message);    }});// Save state when the user attempts to close or reload the windowwindow.addEventListener('beforeunload', (e: BeforeUnloadEvent) => {    try {        saveState();    } catch (err) {        console.error('Failed to save state on beforeunload:', err);    }    // If you want to prompt the user to confirm leaving (browser-dependent), uncomment:    // e.preventDefault();    // e.returnValue = '';});// Tab switchingtabsContainer.addEventListener('click', (e: MouseEvent) => {    const target = e.target as HTMLElement;        if (target.classList.contains('tab-close')) {        const tab = target.parentElement as HTMLElement;        const tabId = tab.dataset.tab;        if (tabId == 'upload') return;        console.debug('Closing tab', tabId);        tab.remove();        document.querySelector(`[data-content="${tabId}"]`)?.remove();        storage.removeItem(`analysis-${tabId}`).catch(err => console.error('Failed to remove analysis from storage:', err));                // Activate upload tab if current was closed        if (tab.classList.contains('active')) {            switchTab('upload');        }        saveState();        e.stopPropagation();    } else if (target.classList.contains('tab')) {        const tabId = target.dataset.tab;        if (tabId) {            switchTab(tabId);        }    }});function switchTab(tabId: string): void {    document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));    document.querySelectorAll('.tab-content').forEach(c => c.classList.remove('active'));        document.querySelector(`[data-tab="${tabId}"]`)?.classList.add('active');    document.querySelector(`[data-content="${tabId}"]`)?.classList.add('active');}analyzeUploadBtn.addEventListener('click', async () => {    const responseFile = responseFileUploadInput.files?.[0];    const referenceFile = referenceFileUploadInput.files?.[0];    if (!responseFile) return;    analyzeUploadBtn.disabled = true;    analyzeUploadBtn.textContent = 'Analyzing...';    try {        const responseData = await audio.loadAudioFile(responseFile);        const referenceData = referenceFile ? await audio.loadAudioFile(referenceFile) : null;        createAnalysisTab(            responseData.applyGain(1 / 16384),            referenceData ? referenceData.applyGain(1 / 16384) : null,            responseFile.name,            referenceFile?.name || null        );    } catch (error) {        alert('Error analyzing files: ' + (error as Error).message);    } finally {        analyzeUploadBtn.disabled = false;        analyzeUploadBtn.textContent = 'Analyze Frequency Response';    }});analyzePolarBtn.addEventListener('click', async () => {    const referenceFile = polarReferenceFileInput.files?.[0];    if (!referenceFile) return;    const measurements = getPolarMeasurements();    if (measurements.length === 0) {        alert('Please add at least one measurement (angle + file).');        return;    }    const oldText = analyzePolarBtn.textContent || 'Analyze Polar Directivity';    analyzePolarBtn.disabled = true;    analyzePolarBtn.textContent = 'Analyzing...';    try {        const referenceData = (await audio.loadAudioFile(referenceFile)).applyGain(1 / 16384);        const loaded = await Promise.all(            measurements.map(async (m) => ({                angleDeg: m.angleDeg,                audio: (await audio.loadAudioFile(m.file)).applyGain(1 / 16384),            }))        );        loaded.sort((a, b) => a.angleDeg - b.angleDeg);        const responseAudios = loaded.map((x) => x.audio);        const anglesDeg = loaded.map((x) => x.angleDeg);        createDirectivityPlotTab(responseAudios, referenceData, anglesDeg);    } catch (error) {        alert('Error analyzing polar files: ' + (error as Error).message);    } finally {        analyzePolarBtn.textContent = oldText;        updatePolarAnalyzeEnabled();    }});function createAnalysisTab(responseData: Audio, referenceData: Audio | null, filename: string, referenceFilename: string | null): void {    tabCounter++;    const tabId = `analysis-${tabCounter}`;    let shortName = filename.length > 20 ? filename.substring(0, 17) + '...' : filename;    if (referenceFilename != null) {        const shortReferenceName = referenceFilename?.length > 20 ? referenceFilename.substring(0, 17) + '...' : referenceFilename;        shortName += ' / ' + shortReferenceName;    }    // Create tab button    const tab = document.createElement('button');    tab.className = 'tab tab-closable';    tab.dataset.tab = tabId;    tab.innerHTML = `<span class="tab-icon-analysis"></span>${shortName} <span class="tab-close">✕</span>`;    tabsInnerContainer.appendChild(tab);        // Create tab content    const content = document.createElement('div');    content.className = 'tab-content';    content.dataset.content = tabId;    content.innerHTML = `    <!-- nav class="tab-menu-bar">                <div>                    <label for="smoothing-${tabId}">Smoothing</label>                    <select id="smoothing-${tabId}" class="smoothing-select" aria-label="Smoothing factor">                        <option value="0">None</option>                        <option value="1/3">1/3 octave</option>                        <option value="1/6" selected>1/6 octave</option>                        <option value="1/12">1/12 octave</option>                        <option value="1/24">1/24 octave</option>                        <option value="1/48">1/48 octave</option>                    </select>                </div>            </nav> <h5 class="text-xs italic text-gray-600">Frequency Response Analysis of ${filename}${referenceFilename ? ' / ' + referenceFilename : ''}</h5 -->        <button class="sidecar-toggle" id="sidebar-toggle-${tabId}" title="Toggle Sidecar">Open settings pane</button>        <div class="flex h-full">            <div class="flex-none w-86 border-r border-[#ddd] p-2 relative sidecar" style="transition:50ms linear;">                <div class="section">                    <div class="title">Settings</div>                    <p><i>There are no settings for this analysis.</i></p>                </div>                <div class="section">                    <div class="title">Plots</div>                    <ul class="list" id="plot-list-${tabId}">                        <!--li><input type="checkbox" id="checkbox-magnitude-${tabId}" alt="show/hide" checked><label for="checkbox-magnitude-${tabId}">Magnitude</label></li>                        <li><input type="checkbox" id="checkbox-phase-${tabId}" alt="show/hide" checked><label for="checkbox-phase-${tabId}">Phase</label></li>                        <li><input type="checkbox" id="checkbox-ir-${tabId}" alt="show/hide" checked><label for="checkbox-ir-${tabId}">Impulse Response</label></li>                        <li><input type="checkbox" id="checkbox-ir-${tabId}" alt="show/hide" disabled><label for="checkbox-ir-${tabId}">Fundamental + Harmonic Distortion</label></li>                        <li><input type="checkbox" id="checkbox-distortion-${tabId}" alt="show/hide" disabled><label for="checkbox-distortion-${tabId}">Distortion</label></li>                        <li><input type="checkbox" id="checkbox-distortion-${tabId}" alt="show/hide" disabled><label for="checkbox-distortion-${tabId}">Sound Pressure Level</label></li>                        <li><input type="checkbox" id="checkbox-deconvoluted-ir-${tabId}" alt="show/hide" disabled><label for="checkbox-deconvoluted-ir-${tabId}">Deconvoluted Impulse Response</label></li>                        <li><input type="checkbox" id="checkbox-stimulus-waveform-${tabId}" alt="show/hide" disabled><label for="checkbox-stimulus-waveform-${tabId}">Stimulus Waveform</label></li>                        <li><input type="checkbox" id="checkbox-recorded-waveform-${tabId}" alt="show/hide" disabled><label for="checkbox-recorded-waveform-${tabId}">Recorded Waveform</label></li>                        <li><input type="checkbox" id="checkbox-recorded-noise-floor-${tabId}" alt="show/hide" disabled><label for="checkbox-recorded-noise-floor-${tabId}">Recorded Noise Floor</label></li>                        <li><input type="checkbox" id="checkbox-target-curve-${tabId}" alt="show/hide" disabled><label for="checkbox-target-curve-${tabId}">Target Curve<button class="float-right text-xs cursor-pointer" style="color: #bbb; padding-top: 3px">Set</button></label></li-->                    </ul>                </div>                <div class="section">                    <div class="title">Properties</div>                    <p><i>There are no properties for this analysis.</i></p>                </div>                <div id="resize-handle" class="resize-handle"></div>            </div>            <div class="flex-1 main-content">                <div class="grid grid-cols-6 gap-[1px] bg-[#ddd] border-b border-[#ddd] plot-outer">                </div>            </div>        </div>                   `;    tabContents.appendChild(content);    // Switch to new tab    switchTab(tabId);    // Compute and plot FFTs    const responseSamples = responseData.getChannelData(0);        const responseFFT = computeFFT(responseSamples);    const smoothedResponseFFT = smoothFFT(responseFFT, 1/6, 1/48);    let referenceSamples = Float32Array.from([]);    plot(        [            {x: responseFFT.frequency, y: db(responseFFT.magnitude), name: 'Recorded signal', line: { color: '#0366d666', width: 0.75}},            {x: smoothedResponseFFT.frequency, y: db(smoothedResponseFFT.magnitude), name: 'Recorded signal (Smoothed)', line: { color: '#0366d6', width: 1.5 }}        ],         tabId,         'Recorded Spectrum',         'Frequency (Hz)',         'Amplitude (dBFS)',        {type: 'log', range: [Math.log10(20), Math.log10(20000)]},         {range: [-85, 5]},        {},         true    );    plot(        [            {x: linspace(0, responseSamples.length/48000, responseSamples.length), y: responseSamples, name: 'Recorded signal', line: { color: '#0366d6ff', width: 0.75}}        ],        tabId,         'Recorded Waveform',         'Time (s)',         'Amplitude',        {},         {},         {},         true    );    if (referenceData) {        referenceSamples = referenceData.getChannelData(0);        const referenceFFT: FFTResult = computeFFT(referenceSamples);        const smoothedReferenceFFT = smoothFFT(referenceFFT, 1/6, 1/48);        plot(            [                {x: referenceFFT.frequency, y: db(referenceFFT.magnitude), name: 'Stimulus signal', line: { color: '#0366d666', width: 0.75}},                {x: smoothedReferenceFFT.frequency, y: db(smoothedReferenceFFT.magnitude), name: 'Stimulus signal (Smoothed)', line: { color: '#0366d6', width: 1.5 }}            ],             tabId,             'Stimulus Spectrum',             'Frequency (Hz)',             'Amplitude (dBFS)',            {type: 'log', range: [Math.log10(20), Math.log10(20000)]},             {range: [-85, 5]},            {},             true        );        plot(            [                {x: linspace(0, referenceSamples.length/48000, referenceSamples.length), y: referenceSamples, name: 'Stimulus signal', line: { color: '#0366d6ff', width: 0.75}}            ],            tabId,             'Stimulus Waveform',             'Time (s)',             'Amplitude',            {},             {},             {},             true        );        const ir: ImpulseResponseResult = twoChannelImpulseResponse(responseSamples, referenceSamples);        const farina = new Farina(referenceSamples, 20, 20000, 48000);        const farina_ir: ImpulseResponseResult = farina.deconvolvedResponse(responseSamples);        plotDistortion(farina, 0.1, 5, tabId);        plotTHD(farina, 0.1, 5, tabId);                    console.log('Impulse response peak at', farina.lag_of_harmonic(2));        plot(            [                {x: ir.t, y:ir.ir, type: 'scatter', mode: 'lines', name: 'Dual-FFT Impulse Response', line: { color: COLORS[0], width: 0.75}}            ],            tabId,            'Impulse Response',            'Time (s)',            'Amplitude',            {},            {},            {},            false        );        plot(            [                   {x: [-max(farina_ir.t), max(farina_ir.t)], y: [-200, -200], showlegend: false},                {x: farina_ir.t, y:db(farina_ir.ir.map(x => Math.abs(x))), type: 'scatter', mode: 'lines', fill: 'tonexty', name: 'Farina Impulse Response', line: { color: COLORS[0], width: 0.75}, fillcolor: COLORS[0]},                {x: [- 0.05, - 0.05], y: [-999, 999], type: 'scatter', mode: 'lines', name: 'Fundamental window start', line: { color: '#00000033', width: 0.75}, hoverinfo: 'skip', showlegend: false},                {x: [0.05, 0.05], y: [-999, 999], type: 'scatter', mode: 'lines', name: 'Fundamental window end', line: { color: '#00000033', width: 0.75}, hoverinfo: 'skip', showlegend: false},                {x: [-farina.lag_of_harmonic(2) - 0.05, -farina.lag_of_harmonic(2) - 0.05], y: [-999, 999], type: 'scatter', mode: 'lines', name: 'H2 window start', line: { color: '#00000033', width: 0.75}, hoverinfo: 'skip', showlegend: false},                {x: [-farina.lag_of_harmonic(2) + 0.05, -farina.lag_of_harmonic(2) + 0.05], y: [-999, 999], type: 'scatter', mode: 'lines', name: 'H2 window end', line: { color: '#00000033', width: 0.75}, hoverinfo: 'skip', showlegend: false},                {x: [-farina.lag_of_harmonic(3) - 0.05, -farina.lag_of_harmonic(3) - 0.05], y: [-999, 999], type: 'scatter', mode: 'lines', name: 'H3 window start', line: { color: '#00000033', width: 0.75}, hoverinfo: 'skip', showlegend: false},                {x: [-farina.lag_of_harmonic(3) + 0.05, -farina.lag_of_harmonic(3) + 0.05], y: [-999, 999], type: 'scatter', mode: 'lines', name: 'H3 window end', line: { color: '#00000033', width: 0.75}, hoverinfo: 'skip', showlegend: false},                {x: [-farina.lag_of_harmonic(4) - 0.05, -farina.lag_of_harmonic(4) - 0.05], y: [-999, 999], type: 'scatter', mode: 'lines', name: 'H4 window start', line: { color: '#00000033', width: 0.75}, hoverinfo: 'skip', showlegend: false},                {x: [-farina.lag_of_harmonic(4) + 0.05, -farina.lag_of_harmonic(4) + 0.05], y: [-999, 999], type: 'scatter', mode: 'lines', name: 'H4 window end', line: { color: '#00000033', width: 0.75}, hoverinfo: 'skip', showlegend: false},                {x: [-farina.lag_of_harmonic(5) - 0.05, -farina.lag_of_harmonic(5) - 0.05], y: [-999, 999], type: 'scatter', mode: 'lines', name: 'H5 window start', line: { color: '#00000033', width: 0.75}, hoverinfo: 'skip', showlegend: false},                {x: [-farina.lag_of_harmonic(5) + 0.05, -farina.lag_of_harmonic(5) + 0.05], y: [-999, 999], type: 'scatter', mode: 'lines', name: 'H5 window end', line: { color: '#00000033', width: 0.75}, hoverinfo: 'skip', showlegend: false},                {x: [-farina.lag_of_harmonic(6) - 0.05, -farina.lag_of_harmonic(6) - 0.05], y: [-999, 999], type: 'scatter', mode: 'lines', name: 'H6 window start', line: { color: '#00000033', width: 0.75}, hoverinfo: 'skip', showlegend: false},                {x: [-farina.lag_of_harmonic(6) + 0.05, -farina.lag_of_harmonic(6) + 0.05], y: [-999, 999], type: 'scatter', mode: 'lines', name: 'H6 window end', line: { color: '#00000033', width: 0.75}, hoverinfo: 'skip', showlegend: false},                {x: [-farina.lag_of_harmonic(7) - 0.05, -farina.lag_of_harmonic(7) - 0.05], y: [-999, 999], type: 'scatter', mode: 'lines', name: 'H7 window start', line: { color: '#00000033', width: 0.75}, hoverinfo: 'skip', showlegend: false},                {x: [-farina.lag_of_harmonic(7) + 0.05, -farina.lag_of_harmonic(7) + 0.05], y: [-999, 999], type: 'scatter', mode: 'lines', name: 'H7 window end', line: { color: '#00000033', width: 0.75}, hoverinfo: 'skip', showlegend: false},                            ],            tabId,            'Deconvolved Response',            'Time (s)',            'Amplitude',            { range: [-1, 1]},            { range: [-150, 10]},            {},            false        );        const transferFunction = computeFFTFromIR(ir);        // const dreferenceFFT = twoChannelFFT(responseData.data, referenceSamples, nextPow2(referenceSamples.length), -5627);        const smoothedFreqResponse = smoothFFT(transferFunction, 1/6, 1/48);                const gd = groupDelays(transferFunction, 1000);        plot(            [                {x: transferFunction.frequency, y: db(transferFunction.magnitude), name: 'Magnitude', line: { color: '#0366d666', width: 0.75}},                {x: smoothedFreqResponse.frequency, y: db(smoothedFreqResponse.magnitude), name: 'Magnitude (Smoothed)', line: { color: '#0366d6', width: 1.5 }}            ],             tabId,             'Transfer Function',             'Frequency (Hz)',             'Amplitude (dBFS)',            {type: 'log', range: [Math.log10(20), Math.log10(20000)]},             {range: [-85, 5]},            {},             false        );                plot(            [                {x: transferFunction.frequency, y: transferFunction.phase, name: 'Phase', line: { color: '#0366d666', width: 0.75}},                {x: smoothedFreqResponse.frequency, y: smoothedFreqResponse.phase, name: 'Phase (Smoothed)', line: { color: '#0366d6', width: 1.5 }}            ],             tabId,             'Phase',             'Frequency (Hz)',             'Amplitude (dBFS)',            {type: 'log', range: [Math.log10(20), Math.log10(20000)]},             {range: [-720, 720]},             {},             false        );        plot(            [                {x: transferFunction.frequency, y: gd, name: 'Group Delay', line: { color: COLORS[0], width: 1.5, dash: 'dot' }}            ],             tabId,             'Group Delay',             'Frequency (Hz)',             'Group Delay (ms)',            {type: 'log', range: [Math.log10(20), Math.log10(20000)]},             {range: [-20, 20]},             {},             false        );        // --- Colormap / Spectrogram (Plotly heatmap) ---        (() => {            const sr =                (responseData as any).sampleRate ??                (referenceData as any)?.sampleRate ??                48000;            const n = responseSamples.length;            if (n < 4096) return;            const windowSize = 2048;            const targetFrames = 320;            const minHop = 256;            const rawFrames = Math.max(1, Math.floor((n - windowSize) / minHop) + 1);            const hop =                rawFrames > targetFrames                    ? Math.max(minHop, Math.ceil((n - windowSize) / targetFrames))                    : minHop;            const frames = Math.max(1, Math.floor((n - windowSize) / hop) + 1);            // Hann window            const win = new Float32Array(windowSize);            for (let i = 0; i < windowSize; i++) {                win[i] = 0.5 * (1 - Math.cos((2 * Math.PI * i) / (windowSize - 1)));            }            // Determine frequency bins from first frame            const firstFrame = new Float32Array(windowSize);            firstFrame.set(responseSamples.subarray(0, windowSize));            for (let i = 0; i < windowSize; i++) firstFrame[i] *= win[i];            const firstFFT = computeFFT(firstFrame);            const freqs = Array.from(firstFFT.frequency);            const bins = freqs.length;            // z[freqIndex][timeIndex]            const z: number[][] = Array.from({ length: bins }, () => []);            const times: number[] = [];            for (let frame = 0; frame < frames; frame++) {                const start = frame * hop;                const slice = responseSamples.subarray(start, start + windowSize);                const windowed = new Float32Array(windowSize);                windowed.set(slice);                for (let i = 0; i < windowSize; i++) windowed[i] *= win[i];                const fft = computeFFT(windowed);                const magDb = db(fft.magnitude);                for (let k = 0; k < bins; k++) {                    z[k].push(magDb[k]);                }                // Use center-of-window time for x axis                times.push((start + windowSize / 2) / sr);            }            plot(                [                    {                        type: 'heatmap',                        x: times,                        y: freqs,                        z,                        colorscale: 'Electric',                        zmin: -120,                        zmax: 0,                        colorbar: { title: 'dBFS' }                    } as any                ],                tabId,                'Recorded Spectrogram',                'Time (s)',                'Frequency (Hz)',                {},                { type: 'log', range: [Math.log10(20), Math.log10(20000)] },                { margin: { l: 60, r: 20, t: 40, b: 50 } },                false            );        })();    }    saveState();    // Persist analysis using IndexedDB (mirrored to sessionStorage for compatibility)    storage.setItem(`${tabId}`, JSON.stringify({        filename,        referenceFilename,        responseSamples: Array.from(responseSamples),        referenceSamples: referenceSamples.length > 0 ? Array.from(referenceSamples) : null,    })).catch(err => console.error('Failed to persist analysis:', err));    function initResize(e: MouseEvent): void {        e.preventDefault();        window.addEventListener('mousemove', resize, false);        window.addEventListener('mouseup', stopResize, false);        console.log('Init resize');        document.body.style.cursor = 'col-resize';    }    function resize(e: MouseEvent): void {        const container = content.querySelector<HTMLElement>('.flex')!;        const handle = document.getElementById('resize-handle')?.parentElement!;        const rect = container.getBoundingClientRect();        const newWidth = e.clientX - rect.left;        if (newWidth > 150 && newWidth < rect.width - 150) {            handle.style.width = `${newWidth}px`;        }    }    function stopResize(): void {        window.removeEventListener('mousemove', resize, false);        window.removeEventListener('mouseup', stopResize, false);        window.dispatchEvent(new Event('resize'));        document.body.style.cursor = 'default';    }           document.getElementById('resize-handle')?.addEventListener('mousedown', initResize, false);}function createDirectivityPlotTab(responseDatas: Audio[], referenceData: Audio, anglesDeg?: number[]): void {    if (responseDatas.length === 0 || referenceData.length === 0) return;    tabCounter++;    const directivityTabId = `directivity-${tabCounter}`;    const shortName = `Directivity (${responseDatas.length})`;    const tab = document.createElement('button');    tab.className = 'tab tab-closable';    tab.dataset.tab = directivityTabId;    tab.innerHTML = `<span class="tab-icon-analysis"></span>${shortName} <span class="tab-close">✕</span>`;    tabsInnerContainer.appendChild(tab);    const content = document.createElement('div');
    content.className = 'tab-content';
    content.dataset.content = directivityTabId;
    content.innerHTML = `
        <div class="flex h-full">
        <div class="flex-1 main-content">
            <div class="grid grid-cols-6 gap-[1px] bg-[#ddd] border-b border-[#ddd] plot-outer"></div>
        </div>
        </div>
    `;
    tabContents.appendChild(content);

    switchTab(directivityTabId);

    const useCustomAngles = !!anglesDeg && anglesDeg.length === responseDatas.length;
    const angles = useCustomAngles
        ? anglesDeg!.map(normalizeAngleDeg)
        : responseDatas.map((_, i) => (360 * i) / responseDatas.length);

    const referenceSamples = Float32Array.from(referenceData.getChannelData(0));

    const transfers: FFTResult[] = responseDatas.map((resp) => {
        const len = Math.min(resp.getChannelData(0).length, referenceSamples.length);
        const ir: ImpulseResponseResult = twoChannelImpulseResponse(
        resp.getChannelData(0).subarray(0, len),
        referenceSamples.subarray(0, len)
        );
        return computeFFTFromIR(ir);
    });

    const baseFreq = transfers[0]?.frequency;
    if (!baseFreq || baseFreq.length === 0) return;

    const normHz = 1000;
    let normIdx = 0;
    let best = Number.POSITIVE_INFINITY;
    for (let i = 0; i < baseFreq.length; i++) {
        const d = Math.abs(baseFreq[i] - normHz);
        if (d < best) {
        best = d;
        normIdx = i;
        }
    }
    if (!useCustomAngles) {
        transfers.push(transfers[0]); // close the circle
        angles.push(360); // close the circle
    }

    // z[angleIndex][freqIndex]
    const z: Float32Array[][] = transfers.map((tf) => {
        const magDb = db(tf.magnitude);
        const ref = magDb[normIdx] ?? 0;
        return magDb.map((v) => v - ref);
    });

    plot(
        [
        {
            type: 'heatmap',
            x: Array.from(baseFreq),
            y: angles,
            z,
            colorscale: 'Electric',
            zmin: -40,
            zmax: 5,
            colorbar: { title: 'dB (norm @ 1 kHz)' }
        } as any
        ],
        directivityTabId,
        'Directivity Map',
        'Frequency (Hz)',
        'Angle (deg)',
        { type: 'log', range: [Math.log10(20), Math.log10(20000)] },
        { range: [0, 360] },
        { margin: { l: 60, r: 20, t: 40, b: 50 } },
        false
    );
}

// Save and load state from sessionStorage
function saveState(): void {
    const tabs = Array.from(document.querySelectorAll('.tab[data-tab]')).map(tab => ({
        id: (tab as HTMLElement).dataset.tab,
        name: (tab as HTMLElement).textContent?.replace('×', '').trim()
    }));
    
    storage.setItem('tabs', JSON.stringify(tabs));
    console.log('Saved state with tabs:', tabs);
}

async function loadState(): Promise<void> {
    try {
        const savedTabs = await storage.getItem('tabs');
        if (!savedTabs) return;
        const tabs = JSON.parse(savedTabs);
        console.log('Loading saved tabs:', tabs);

        for (const tab of tabs as { id: string; name: string }[]) {
            // Call createAnalysisTab for each tab
            const raw = await storage.getItem(`${tab.id}`);
            const analysisData = raw ? JSON.parse(raw) : null;
            console.log('Restoring analysis data for tab', tab.id, analysisData);
            if (analysisData) {
                createAnalysisTab(Audio.fromSamples(Float32Array.from(analysisData.responseSamples)), analysisData.referenceSamples ? Audio.fromSamples(Float32Array.from(analysisData.referenceSamples)) : null, analysisData.filename, analysisData.referenceFilename);
            }
        }
        // Tabs will be recreated when user analyzes files again
    } catch (e) {
        console.error('Failed to load saved state:', e);
    }
}

// Load state on page load
loadState();